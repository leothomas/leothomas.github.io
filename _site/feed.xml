<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-18T09:11:22-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Leo Thomas</title><subtitle>Hi, I'm Leo! I am a data and software engineer based in Washington D.C.  I like to work with python and AWS to manage data and extract insights.</subtitle><entry><title type="html">Back-propagation and gradient descent from scratch</title><link href="http://localhost:4000/2020/05/01/neural-networks.html" rel="alternate" type="text/html" title="Back-propagation and gradient descent from scratch" /><published>2020-05-01T11:21:12-04:00</published><updated>2020-05-01T11:21:12-04:00</updated><id>http://localhost:4000/2020/05/01/neural-networks</id><content type="html" xml:base="http://localhost:4000/2020/05/01/neural-networks.html">&lt;p&gt;
    While trying to learn more about neural networks, I had trouble understanding back-propagation
    and gradient descent, because most online articles tend to explain the matrix operations that
    implement these concepts rather than the conepts themselves. While matrix operations are indeed
    the most effective implementations of back-propagation and gradient descent, they're not
    very easy to intuitively understand. As an excrcise, I decided to implement a neural netwok
    using Python classes in order to see how these calculations are executed at the neuron level.
&lt;/p&gt;
&lt;p&gt;
    I also thought it would be interesting to see just how much slower neural networks written in
    OOP are, compared to the standard matrix approach.
&lt;/p&gt;

&lt;h3&gt;Neurons and Synapses &lt;/h3&gt;
&lt;p&gt;
    All neural networks are composed of neurons, which are organized into layers, and synapses which
    connect the neurons from one layer to the neurons of the next:
&lt;/p&gt;
&lt;p&gt;
    &lt;img class=&quot;img-fluid&quot; src=&quot;/img/posts/neural-networks/neural-network-structure.webp&quot;
        alt=&quot;Basic Neural Network&quot;&gt;
    &lt;span class=&quot;caption text-muted&quot;&gt;Visual representation of a basic neural network made up of
        neurons (circles) and synapses (arrows). The neurons labelled A,B,C, and D are part of
        the input layer, those in red are part of the hidden layer and the neuron in green
        is part of the output layer.
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
    I defined neurons with the following attributes and methods:
    &lt;code&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Neuron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; 
    Basic block of neural network. Takes input from the previous 
    layer (or the input data) and provides output to the next layer 
    (or outputs the network's prediction). Updates weights based on 
    it's contribution to the error in the network's prediction.

    ...

    Attributes:
    ----------
    output : float 
        Value to be used as input by the next layer neurons or as the
        network's prediction (if in the last layer).

    bias : float
        Used to determine the neuron's output.
        b in the equation y = mx + b, where mx is the weighted 
        average of the previous layer neurons' outputs and y is the 
        neuron's activation.

    learning_rate : float  
        Modulates how much bias and weights are updated by. Smaller
        values will converge more slowly and more precisely but are
        also more susceptible to local minima (non-optimal 
        solutions). Start large and reduce proportionally to training
        error (~0.1 to ~0.001).

    synapses_in : [Synapse] | None
        Pointers to the previous layer's neurons (None if the neuron
        is part of the network's input layer).

    synapses_out : [Synapse] | None
        Pointers to the next layer's neurons (None if the neuron is
        part of the network's output layer).

    Methods :
    -------
    activate(network_input: float | None) -&amp;gt; None
        Calculates the weighted average of the previous layer's 
        neurons plus its own bias (y = mx + b).

    transfer(x : float) -&amp;gt; float
        Restricts the neuron's activation, calculated above, to the 
        range [0,1].

    calculate_error(expected: float | None) -&amp;gt; None
        Calculates contribution from this neuron to the error in the 
        network's prediction.

    transfer_derivative(x : float) -&amp;gt; float
        Derivative of the function used to restriction the activation
        to [0,1]. Used to calculate the neuron's contribution to the 
        network's error.

    update_weights() -&amp;gt; None
        Updates the weights and biases of this neuron according to 
        its error contribution calculated above.
    &quot;&quot;&quot;&lt;/span&gt;     
        &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;
    My initial approach to connecting the neurons was to use a doubly-linked list structure, where
    each neuron has a list of output neurons and a list of input neurons. Since the connections
    never need to be modified, the burden of swapping pointers in a doubly-linked list is not a
    concern.
&lt;/p&gt;
&lt;p&gt;
    However each connection between two neurons is assigned a weight, which needs to be
    accessed both from the input neuron (during forward propagation) and the output neuron (during
    backwards propagation). The doubly-linked list structure would have forced me to keep track of
    these weights in both the input neuron and the output neuron.
&lt;/p&gt;
&lt;p&gt;
    Updating a weight would have required accessing it in both the input neuron and the output
    neuron. Considering the large quantity of weight update events that occur when training the
    network, this approach would have been both computationally intensive and more error prone, in
    addition to making the code harder to maintain.
&lt;/p&gt;
&lt;p&gt;

    In order to avoid these issues, I created a separate class for synapses, defined as follows:
&lt;/p&gt;
&lt;p&gt;
    &lt;code&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Synapse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Connector between two neurons of different layers
    ... 
    
    Attributes:
    -----------
    neuron_in: Neuron 
        Neuron feeding input into this synapse.
    neuron_out: Neuron
        Neuron which receives input from this synapse.
    weight: float
        Weight of the synapse. Modulates how much of the input into 
        the synapse is passed on to the output neuron.
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;
    I built the network by instantiating layers of neurons, and connecting each neuron within one
    layer to all the neurons of the next layer:
&lt;/p&gt;
&lt;p&gt;
    &lt;code&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neuron&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_neuron&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;synapse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Synapse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;neuron_in&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;neuron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;neuron_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_neuron&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;neuron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;synapses_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;synapse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;target_neuron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;synapses_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;synapse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
    &lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;
    The synapse is added to both the output synapses of the input neuron and the input synapses
    of the output neuron. This way, during forward propagation, the weight from one neuron to the
    next is accessed using:
&lt;/p&gt;
&lt;p&gt;
    &lt;code&gt;
    &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;n&quot;&gt;input_neuron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;synapses_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; 
    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
    &lt;/code&gt;
&lt;/p&gt;
and during backward propagation, the weight from the previous neuron to the current is accessed
using:
&lt;p&gt;
    &lt;code&gt;
    &lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;n&quot;&gt;output_neuron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;synapses_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;
    &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
    &lt;/code&gt;
&lt;/p&gt;

&lt;p&gt;
    As of now, the network can only be configured with fully connected layers; a great next step
    would be to implement a way to configure dropout, convolution, and pooling layers.
&lt;/p&gt;
&lt;h3&gt;
    Forward Propagation
&lt;/h3&gt;
&lt;p&gt;
    With neurons and synapses defined this way, forward propagation becomes a bit clearer.
&lt;/p&gt;
&lt;i&gt;More coming soon!&lt;/i&gt;</content><author><name></name></author><summary type="html">While trying to learn more about neural networks, I had trouble understanding back-propagation and gradient descent, because most online articles tend to explain the matrix operations that implement these concepts rather than the conepts themselves. While matrix operations are indeed the most effective implementations of back-propagation and gradient descent, they're not very easy to intuitively understand. As an excrcise, I decided to implement a neural netwok using Python classes in order to see how these calculations are executed at the neuron level. I also thought it would be interesting to see just how much slower neural networks written in OOP are, compared to the standard matrix approach.</summary></entry><entry><title type="html">Clean Qubits and Dirty Channels</title><link href="http://localhost:4000/2020/05/01/quantum-computing.html" rel="alternate" type="text/html" title="Clean Qubits and Dirty Channels" /><published>2020-05-01T11:21:12-04:00</published><updated>2020-05-01T11:21:12-04:00</updated><id>http://localhost:4000/2020/05/01/quantum-computing</id><content type="html" xml:base="http://localhost:4000/2020/05/01/quantum-computing.html">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;
    type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;
    In my last year at McGill (2017), I did a research project in quantum computation with Dr.
    Michael Hilke, head of the McGill Quantum NanoElectronic’s Laboratory. The point of the project
    was to analyse the effect of random disorder on qubit entanglement.
&lt;/p&gt;
&lt;p&gt;
    Before getting into the simulation itself, I want to briefly go over a couple important concepts
    in quantum computing: &lt;strong&gt;superposition&lt;/strong&gt; and &lt;strong&gt;entanglement&lt;/strong&gt;. (If
    you're already familiar with these, feel free to skip ahead!)
&lt;/p&gt;
&lt;h5&gt;
    Superposition:
&lt;/h5&gt;
&lt;p&gt;
    Superposition is the idea that a quantum particle can exist in multiple states at the same time.
&lt;/p&gt;
&lt;p&gt;
    In a regular computer, the most basic unit of information - called a bit - can be in one of two
    states: 0 or 1.
&lt;/p&gt;
&lt;p&gt;
    In a quantum computer, the most basic unit of information - called a qubit - is a particle in a
    quantum state, that can be described as a linear combination of the two basic states:
    |&amp;nbsp;0&amp;nbsp;&amp;#8250; and |&amp;nbsp;1&amp;nbsp;&amp;#8250;.
&lt;/p&gt;
&lt;p&gt;
    The &quot;&amp;nbsp;|&amp;nbsp;&amp;#8250;&amp;nbsp&quot; notation is a mathematical notation used to describe quantum
    states, called
    &lt;a href=&quot;https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation&quot;&gt;
        BraKet notation - or Dirac Notation
    &lt;/a&gt;. |&amp;nbsp;0&amp;nbsp;&amp;#8250; indicates a quantum state that would collapse to the
    classical state 0 if it were measured, and |&amp;nbsp;1&amp;nbsp;&amp;#8250; indicates a quantum state that
    would collapse to the classical state 1, if it were measured.
&lt;/p&gt;
&lt;p&gt;
    One way to think about quantum particle states it to imagine the particle inside of a box.
    At any given moment, the exact state of the particle cannot be precisely known without measuring
    it. The act of measuring it causes the particle's state to collapse into a classical state (0 or
    1). Without measuring the particle's state, the only thing that &lt;strong&gt;can&lt;/strong&gt; be known
    precisely is the &lt;strong&gt;probability&lt;/strong&gt; of the particle collapsing into one state or the
    other, if were measured at that moment. (This is the concept explained by the
    Schrödinger's cat analogy: a cat is in a box with a radioactive source; since we cannot know
    whether the cat is dead or alive at any given moment, the act of opening the box amounts to
    killing - or saving - the cat.)
&lt;/p&gt;

&lt;p&gt;
    The linear combination, or superposition, of states that describes the state of a quantum
    particle can be represented mathematically as:
&lt;/p&gt;
&lt;p&gt;
    |&amp;nbsp;&amp;#968;&amp;nbsp;&amp;#8250;&amp;nbsp;=&amp;nbsp;&amp;#945;&amp;nbsp;|&amp;nbsp;0&amp;nbsp;&amp;#8250;&amp;nbsp;+&amp;nbsp;&amp;#946;&amp;nbsp;|&amp;nbsp;1&amp;nbsp;&amp;#8250;&amp;nbsp;,
    where &amp;#945; and &amp;#946; are complex numbers.

&lt;/p&gt;
&lt;p&gt;
    If the qubit were measured, it would be found in state |&amp;nbsp;0&amp;nbsp;&amp;#8250; with probability
    |&amp;#945;|&amp;#0178; and in state |&amp;nbsp;1&amp;nbsp;&amp;#8250; with probability |&amp;#946;|&amp;#0178;. The
    probability of finding a qubit in one state or another, if it were measured at that moment, is
    called the qubit's expectation value.
&lt;/p&gt;

&lt;p&gt;
    Here's a graph from my simulation of the expectation value of a single qubit, as it evolves over
    time.
&lt;/p&gt;
&lt;p&gt;
    &lt;img class=&quot;img-fluid&quot; src=&quot;/img/posts/quantum-computing/expectation_values_single.jpg&quot;
        alt=&quot;Qubit Evolution Expectation values&quot;&gt;
    &lt;span class=&quot;caption text-muted&quot;&gt;Expectation values of a qubit. Note that as the probability of
        it being measured in one state goes to 1.0 the probability of it being measured in the
        other state goes to 0. &lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;
    In summary, where a classical bit can map to one of 2 values, superposition allows a qubit
    to exist as an infite number of values (as long as it is not measured). You can think of a
    bit as a light switch and a qubit as pointing to anywhere on surface of a sphere:
&lt;/p&gt;
&lt;p&gt;
    &lt;img class=&quot;img-fluid&quot; src=&quot;/img/posts/quantum-computing/bit-vs-qubit.png&quot; alt=&quot;Bit vs Qubit&quot;&gt;
    &lt;span class=&quot;caption text-muted&quot;&gt;A visual representation of the possible values of bits and
        qubits&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;h5&gt;
        Entanglement:
    &lt;/h5&gt;

    When two qubits are entangled, the state of one qubit affects the sate of another qubit,
    regardless of where the qubits are or how far appart they are.
&lt;/p&gt;
&lt;p&gt;
    To illustrate, take two qubits entangled in an equal superposition, meaning either both are in
    the |&amp;nbsp;0&amp;nbsp;&amp;#8250; state or both are in the |&amp;nbsp;1&amp;nbsp;&amp;#8250; state -
    denoted |&amp;nbsp;00&amp;nbsp;&amp;#8250; and |&amp;nbsp;11&amp;nbsp;&amp;#8250;, respectively. The state of this
    system of qubits is mathematically represented as: &lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;
    | &amp;#968; &amp;#8250; = &lt;sup&gt;1&lt;/sup&gt;/&lt;sub&gt;&amp;#8730;2&lt;/sub&gt; ( |&amp;nbsp;00&amp;nbsp;&amp;#8250; +
    |&amp;nbsp;11&amp;nbsp;&amp;#8250; )
&lt;/p&gt;
&lt;p&gt;
    In this case the states |&amp;nbsp;00&amp;nbsp;&amp;#8250; and |&amp;nbsp;11&amp;nbsp;&amp;#8250; are equally probable
    (( &lt;sup&gt;1&lt;/sup&gt;/&lt;sub&gt;&amp;#8730;2&lt;/sub&gt;)&amp;#0178; = 0.5, the system has a 50% chance of being
    chance of being found in either state).

&lt;/p&gt;
&lt;p&gt;
    If one qubit is found to be in the |&amp;nbsp;0&amp;nbsp;&amp;#8250; state then the other must
    &lt;strong&gt;necessarily&lt;/strong&gt; be in the |&amp;nbsp;0&amp;nbsp;&amp;#8250; state and if one is found to be in
    the |&amp;nbsp;1&amp;nbsp;&amp;#8250; state then the other must be in the |&amp;nbsp;1&amp;nbsp;&amp;#8250; state (since
    the only two possible states of the entagled system are |&amp;nbsp;00&amp;nbsp;&amp;#8250; and
    |&amp;nbsp;11&amp;nbsp;&amp;#8250;&amp;nbsp;). Incredibly, this is true &lt;strong&gt;regardless&lt;/strong&gt; of where the
    qubits are and how far apart they are from each other!
&lt;/p&gt;
&lt;p&gt;
    Entaglement is crucial to &quot;quantum 2.0&quot; technologies that use qubit entanglement to improve the
    precision of their measurements. These technologies include improved atomic clocks that keep
    time with an order of magnitude more precision than current leading clocks, as well as electron
    microscopes and satellites.
&lt;/p&gt;
&lt;p&gt;
    In the case of quantum computers, qubits must be entangled through a channel, such as a proton
    beam or or fiber optic cable. These channels are prone to disorder, which is almost always
    detrimental to the information transfer between the entangled particles. Electromagnetic signals
    from nearby machinery and background radiation are two common culprits. These kinds of disorder
    are called dynamic, or time dependent, and can be modeled mathematically. They are well studied,
    and correction schemes have been developed to mitigate their effects.

&lt;/p&gt;
&lt;p&gt;
    My project was concerned with non-time dependent disorder, or static disorder, such as
    impurities in the air or in a fiber optic cable. Due to their highly random nature, these kinds
    of disorder cannot be modeled mathematically, and are much harder to account for. The goal of my
    project was to develop a python simulation of entangled particles to quantify how their
    information transfer was affected when random disorder is introduced to the entanglement
    channel.
&lt;/p&gt;

&lt;h3&gt;The simulation&lt;/h3&gt;
&lt;p&gt;
    The simulation starts with the famous Schrödinger equation, which describes the states of
    quantum systems. If you are interested in a slightly more in-depth explanation of the quantum
    theory behind the simulation, please feel free to check out the
    &lt;strong&gt; &lt;a href=&quot;https://drive.google.com/file/d/1V2eJxy9jgibWXy9N4vlC5e-BQ0p_6JU2/view&quot;&gt;
            paper&lt;/a&gt;&lt;/strong&gt; itself!
&lt;/p&gt;
&lt;p&gt;
    For the simulation, we use a mathmatical operator called a Hamiltonian (\(\hat{H}\)), to
    represent the quantum particle:
    $$\hat{H} = \begin{bmatrix}
    \varepsilon_{up} &amp;&amp; \tau \\
    \tau^* &amp;&amp; \varepsilon_{down}
    \end{bmatrix}$$
    (\(\varepsilon_{up}\), \(\varepsilon_{down}\) have to do with the energy of the quantum
    particle, and vary between 0 and 1, and \(\tau\) is not very important within the scope of this
    post)
&lt;/p&gt;
&lt;p&gt;
    The Hamiltonian operator makes it straightforward to mathematically represent the particles as
    they become entangled, by using an operation called the Kronecker product (\(\otimes\)). The
    Kronecker product simply multiplies each element of the first matrix by the entire second
    matrix:
    $$\begin{bmatrix}
    a_{11} &amp; a_{12} \\
    a_{11} &amp; a_{22} \\
    \end{bmatrix}
    \otimes
    \begin{bmatrix}
    b_{11} &amp; b_{12} \\
    b_{11} &amp; b_{22} \\
    \end{bmatrix}
    =
    \begin{bmatrix}
    a_{11} {\begin{bmatrix}
    b_{11} &amp; b_{12} \\
    b_{11} &amp; b_{22} \\
    \end{bmatrix}} &amp; a_{12} {\begin{bmatrix}
    b_{11} &amp; b_{12} \\
    b_{11} &amp; b_{22} \\
    \end{bmatrix}} \\
    a_{21} {\begin{bmatrix}
    b_{11} &amp; b_{12} \\
    b_{11} &amp; b_{22} \\
    \end{bmatrix}} &amp; a_{22} {\begin{bmatrix}
    b_{11} &amp; b_{12} \\
    b_{11} &amp; b_{22} \\
    \end{bmatrix}} \\
    \end{bmatrix}$$
&lt;/p&gt;

&lt;p&gt;
    In order to represent two entangled qubits - let's call them A and B - we calculate:
    $$ \hat{H}_A \otimes \hat{H}_B $$

&lt;/p&gt;
&lt;p&gt;
    In order to represent qubit entangled through a channel, we simply construct a chain
    of entangled quantum particles. We Kronecker product to entangle each qubit to the
    next and then consider the ones on either extremity to be the entangled particles we want to
    study, and the rest to be part of the channel.
&lt;/p&gt;
&lt;p&gt;
    Once we have a mathematical representation of the qubits entangled through a channel, we add
    some random disturbance to the entanglement channel and see how the qubits on either end act
    differently from when there is no energy in the channel.
&lt;/p&gt;
&lt;p&gt;
    The Hamiltonian operator that represents the qubits and their entanglement channel is used to
    find a solution to the Schrödinger equation (using a propagator known as the
    &lt;a href=&quot;https://sites.math.washington.edu/~morrow/336_11/papers/jeff.pdf&quot;&gt; Green function&lt;/a&gt;).
    This solution is energy dependent, meaning it defines state of the system with
    respect to the particles' energy. In order to describe the system's state as it
    evolves over time we use an operation known as a
    &lt;a href=&quot;https://en.wikipedia.org/wiki/Fourier_transform&quot;&gt;Fourier Transform&lt;/a&gt; to describe the
    system's state as it evolves over time. The result of this transform is the expectation values
    of two entangled qubits time:
&lt;/p&gt;
&lt;p&gt;
    &lt;img class=&quot;img-fluid&quot; src=&quot;/img/posts/quantum-computing/coupled-qubits-expectation-values.jpg&quot;
        alt=&quot;Expectation values of Entangled Qubits&quot;&gt;
    &lt;span class=&quot;caption text-muted&quot;&gt;Time evolution of the expectation values of two entangled
        qubits
    &lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;
    Notice that each qubit's expectation values oscilate between 0 and 1.0, at a different rate.
    These oscillations will come in and out of phase and at certain times, will be anti-phased (one
    qubit's expectation value reaches 1.0 at the same time as the other reaches 0). When the
    expectation values are anti-phased their oscillations become restricted, as we see around
    time t=25. If the particles were not entangled, their expectation values would simultaneously
    reach 100% probability of being found in opposite states (|&amp;nbsp;10&amp;nbsp;&amp;#8250; or
    |&amp;nbsp;01&amp;nbsp;&amp;#8250;). However, since such states are prohibited by the entanglement, the
    expectation values of both qubits are restricted to 0.5 around time t=25. The expectation values
    recover their full oscillation as the system comes out of this anti-phased state, and the
    expectation values no longer simultaneously reach 100% probability of being found
    in opposite states.
&lt;/p&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;
    In this project, I tackled the question: how does static disorder in quantum entanglement
    channels
    affect information transfer between qubits? The answer: it does.
&lt;/p&gt;
&lt;p&gt;
    The graph below shows the average difference between the expectation values of qubits entangled
    through a disordered channel and those of a non-entangled qubit, at different disorder ranges
    (eg: a disorder range of 0.25 means the qubits forming the entanglement channel have been
    assigned random energy values between -0.25 and 0.25). We notice that as that disorder range
    increases, the qubit behaves less and less like an entangled qubit and more like an isolated
    qubit.
&lt;/p&gt;
&lt;p&gt;
    &lt;img class=&quot;img-fluid&quot; src=&quot;/img/posts/quantum-computing/average_difference_entangled.jpeg&quot;
        alt=&quot;Average Difference&quot;&gt;
    &lt;span class=&quot;caption text-muted&quot;&gt;The average difference of qubit expectation values over 31
        trials. Notice a sharp drop off at the disorder range value 0.5 (marked with a red line).
        This is likely due to the fact that at that point disorder range (-0.5 to 0.5) exceeds the
        channel width (1.0), and starts blocksing the qubits from communicating effectively,
        eventually leading to a system where the qubits are almost entorely isolated (at disorder
        range 1.5)
    &lt;/span&gt;
&lt;/p&gt;

&lt;p&gt;
    While this may seem like a rather underwhelming result because it's exactly what we would
    intuitively expect, this project created and validated a framework for modeling the
    behaviours of entangled quantum particles. This kind of framework can be very useful in many
    other quantum computing research applications.
&lt;/p&gt;
&lt;p&gt;
    Secondly, we can't begin to quantify the effect of disorder on quantum systems if we have not
    proved that it does affect the system in the first place. Simulations like these are much
    cheaper and easier to run than physical experiments involving quantum particles, and are a great
    starting point for further research.
&lt;/p&gt;
&lt;p&gt;
    In the case of this project a great next step would to be find other metrics to quantify the
    difference in behaviour between the entangled qubit and the isolated qubit, other than just the
    difference in their expectation values. Another good subject to further research would be the
    rate of decoupling in quantum systems with large disorder values.
&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;
    My experience with quantum computing was incredibly rewarding (albeit stressful due to time
    constraints). I was very proud to achieve some results, as basic and obvious as they may have
    beenm, they were correct and valid! Ever since I have been fascinated with the idea of modeling
    the world around us.
&lt;/p&gt;
&lt;p&gt;
    Thanks for reading! Please feel free to check out the
    &lt;strong&gt;
        &lt;a href=&quot;https://drive.google.com/file/d/1V2eJxy9jgibWXy9N4vlC5e-BQ0p_6JU2/view&quot;&gt;paper&lt;/a&gt;
    &lt;/strong&gt;
    for more info on the simulation and the physics involves! If you have any comments, questions or
    suggestions regarding this post, please &lt;a href=&quot;/contact.html&quot;&gt; get in touch&lt;/a&gt;!
&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>